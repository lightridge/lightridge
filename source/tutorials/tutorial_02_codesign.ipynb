{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ExV321SwkMS6"
   },
   "outputs": [],
   "source": [
    "#it is recommended to run in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE3fZBVUbkSg",
    "outputId": "66a50539-a3cf-497e-93a6-2ecc35ee2e61"
   },
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ lightbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GCX7ZXDGbR60"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from time import time\n",
    "import random\n",
    "import pathlib\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import lightbridge.utils as utils\n",
    "import lightbridge.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tIQcxzX3bYCE"
   },
   "outputs": [],
   "source": [
    "class NetCodesign(torch.nn.Module):\n",
    "    def __init__(self, phase_func, intensity_func, wavelength=5.32e-7, pixel_size=0.000036, batch_norm=False, sys_size = 200, distance=0.1, num_layers=2, precision=256, amp_factor=6):\n",
    "        super(NetCodesign, self).__init__()\n",
    "        self.amp_factor = amp_factor\n",
    "        self.size = sys_size\n",
    "        self.distance = distance\n",
    "        self.phase_func = phase_func.cuda()\n",
    "        self.intensity_func = intensity_func.cuda()\n",
    "        self.wavelength = wavelength\n",
    "        self.pixel_size = pixel_size\n",
    "        #self.phase_func = phase_func\n",
    "        #self.intensity_func = intensity_func\n",
    "        self.diffractive_layers = torch.nn.ModuleList([layers.DiffractiveLayer(self.phase_func, self.intensity_func, size=self.size,\n",
    "                                                    wavelength = self.wavelength, pixel_size = self.pixel_size,\n",
    "                                distance=self.distance, amplitude_factor = amp_factor, phase_mod=True) for _ in range(num_layers)])\n",
    "        self.last_diffraction = layers.DiffractiveLayer(None, None, size=self.size, distance=self.distance, phase_mod=False)\n",
    "        # 200 by 200 system siz det designe\n",
    "        self.detector = layers.Detector(start_x = [46,46,46], start_y = [46,46,46], det_size = 20,\n",
    "                                        gap_x = [19,20], gap_y = [27, 12, 27])\n",
    "    def forward(self, x):\n",
    "        for index, layer in enumerate(self.diffractive_layers):\n",
    "            x = layer(x)\n",
    "        x = self.last_diffraction(x)\n",
    "        output = self.detector(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t7cFHQlVbzDi"
   },
   "outputs": [],
   "source": [
    "def train(model,train_dataloader, val_dataloader,input_padding, lambda1):\n",
    "    criterion = torch.nn.MSELoss(reduction='sum').cuda()\n",
    "    print('training starts.')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.7)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=20, gamma=0.5)\n",
    "    for epoch in range(1, 21):\n",
    "        log = [epoch]\n",
    "        model.train()\n",
    "        train_len = 0.0\n",
    "        train_running_counter = 0.0\n",
    "        train_running_loss = 0.0\n",
    "        tk0 = tqdm(train_dataloader, ncols=150, total=int(len(train_dataloader)))\n",
    "        for train_iter, train_data_batch in enumerate(tk0):\n",
    "            train_images, train_labels = utils.data_to_cplex(train_data_batch)\n",
    "            train_outputs = model(train_images)\n",
    "            train_loss_ = lambda1 * criterion(train_outputs, train_labels)\n",
    "            train_counter_ = torch.eq(torch.argmax(train_labels, dim=1), torch.argmax(train_outputs, dim=1)).float().sum()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss_.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_len += len(train_labels)\n",
    "            train_running_loss += train_loss_.item()\n",
    "            train_running_counter += train_counter_\n",
    "\n",
    "            train_loss = train_running_loss / train_len\n",
    "            train_accuracy = train_running_counter / train_len\n",
    "\n",
    "            tk0.set_description_str('Epoch {}/{} : Training'.format(epoch, 20))\n",
    "            tk0.set_postfix({'Train_Loss': '{:.2f}'.format(train_loss), 'Train_Accuracy': '{:.5f}'.format(train_accuracy)})\n",
    "        scheduler.step()\n",
    "        log.append(train_loss)\n",
    "        log.append(train_accuracy)\n",
    "\n",
    "        with open('./result.csv', 'a', newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(log)\n",
    "        val_loss, val_accuracy = eval(model, val_dataloader, epoch,input_padding)\n",
    "        log.append(val_loss)\n",
    "        log.append(val_accuracy)\n",
    "    return train_loss, train_accuracy, val_loss, val_accuracy, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pvmPrHx3b6Jb"
   },
   "outputs": [],
   "source": [
    "def eval(model, val_dataloader, epoch, input_padding):\n",
    "    criterion = torch.nn.MSELoss(reduction='sum').cuda()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_len = 0.0\n",
    "        val_running_counter = 0.0\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "        tk1 = tqdm(val_dataloader, ncols=100, total=int(len(val_dataloader)))\n",
    "        for val_iter, val_data_batch in enumerate(tk1):\n",
    "            val_images, val_labels = utils.data_to_cplex(val_data_batch)\n",
    "            val_outputs = model(val_images)\n",
    "\n",
    "            val_loss_ = criterion(val_outputs, val_labels)\n",
    "            val_counter_ = torch.eq(torch.argmax(val_labels, dim=1), torch.argmax(val_outputs, dim=1)).float().sum()\n",
    "\n",
    "            val_len += len(val_labels)\n",
    "            val_running_loss += val_loss_.item()\n",
    "            val_running_counter += val_counter_\n",
    "\n",
    "            val_loss = val_running_loss / val_len\n",
    "            val_accuracy = val_running_counter / val_len\n",
    "\n",
    "            tk1.set_description_str('Epoch {}/{} : Validating'.format(epoch, 20))\n",
    "            tk1.set_postfix({'Val_Loss': '{:.5f}'.format(val_loss), 'Val_Accuarcy': '{:.5f}'.format(val_accuracy)})\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EKqVWpic1OK",
    "outputId": "db1e15ff-529c-48c4-f13f-f380a1630d59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training and testing on MNIST10 dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  2.94it/s, Train_Loss=0.76, Train_Accuracy=0.58450]\n",
      "Epoch 1/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.40it/s, Val_Loss=0.39255, Val_Accuarcy=0.786\n",
      "Epoch 2/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.26, Train_Accuracy=0.85677]\n",
      "Epoch 2/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.39it/s, Val_Loss=0.15603, Val_Accuarcy=0.913\n",
      "Epoch 3/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.02it/s, Train_Loss=0.16, Train_Accuracy=0.90960]\n",
      "Epoch 3/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.33it/s, Val_Loss=0.16081, Val_Accuarcy=0.911\n",
      "Epoch 4/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.13, Train_Accuracy=0.92610]\n",
      "Epoch 4/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.33it/s, Val_Loss=0.11417, Val_Accuarcy=0.936\n",
      "Epoch 5/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.11, Train_Accuracy=0.93848]\n",
      "Epoch 5/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.40it/s, Val_Loss=0.09560, Val_Accuarcy=0.947\n",
      "Epoch 6/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.02it/s, Train_Loss=0.10, Train_Accuracy=0.94412]\n",
      "Epoch 6/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.30it/s, Val_Loss=0.09349, Val_Accuarcy=0.948\n",
      "Epoch 7/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  2.98it/s, Train_Loss=0.09, Train_Accuracy=0.95152]\n",
      "Epoch 7/20 : Validating: 100%|█| 17/17 [00:04<00:00,  3.41it/s, Val_Loss=0.07476, Val_Accuarcy=0.959\n",
      "Epoch 8/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.08, Train_Accuracy=0.95818]\n",
      "Epoch 8/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.36it/s, Val_Loss=0.07958, Val_Accuarcy=0.955\n",
      "Epoch 9/20 : Training: 100%|███████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.07, Train_Accuracy=0.96073]\n",
      "Epoch 9/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.36it/s, Val_Loss=0.07538, Val_Accuarcy=0.958\n",
      "Epoch 10/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:32<00:00,  3.04it/s, Train_Loss=0.06, Train_Accuracy=0.96640]\n",
      "Epoch 10/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.34it/s, Val_Loss=0.05988, Val_Accuarcy=0.96\n",
      "Epoch 11/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  3.02it/s, Train_Loss=0.06, Train_Accuracy=0.96855]\n",
      "Epoch 11/20 : Validating: 100%|█| 17/17 [00:04<00:00,  3.41it/s, Val_Loss=0.06137, Val_Accuarcy=0.96\n",
      "Epoch 12/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:32<00:00,  3.04it/s, Train_Loss=0.05, Train_Accuracy=0.97133]\n",
      "Epoch 12/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.37it/s, Val_Loss=0.05352, Val_Accuarcy=0.96\n",
      "Epoch 13/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  2.99it/s, Train_Loss=0.04, Train_Accuracy=0.97608]\n",
      "Epoch 13/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.33it/s, Val_Loss=0.06316, Val_Accuarcy=0.96\n",
      "Epoch 14/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  3.03it/s, Train_Loss=0.04, Train_Accuracy=0.97662]\n",
      "Epoch 14/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.37it/s, Val_Loss=0.05705, Val_Accuarcy=0.96\n",
      "Epoch 15/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  3.00it/s, Train_Loss=0.04, Train_Accuracy=0.97948]\n",
      "Epoch 15/20 : Validating: 100%|█| 17/17 [00:04<00:00,  3.40it/s, Val_Loss=0.05436, Val_Accuarcy=0.96\n",
      "Epoch 16/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  3.01it/s, Train_Loss=0.03, Train_Accuracy=0.98148]\n",
      "Epoch 16/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.31it/s, Val_Loss=0.05156, Val_Accuarcy=0.97\n",
      "Epoch 17/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  2.97it/s, Train_Loss=0.03, Train_Accuracy=0.98347]\n",
      "Epoch 17/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.35it/s, Val_Loss=0.04663, Val_Accuarcy=0.97\n",
      "Epoch 18/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  2.95it/s, Train_Loss=0.03, Train_Accuracy=0.98562]\n",
      "Epoch 18/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.39it/s, Val_Loss=0.04783, Val_Accuarcy=0.97\n",
      "Epoch 19/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  2.97it/s, Train_Loss=0.02, Train_Accuracy=0.98678]\n",
      "Epoch 19/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.32it/s, Val_Loss=0.04713, Val_Accuarcy=0.97\n",
      "Epoch 20/20 : Training: 100%|██████████████████████████████████████████████| 100/100 [00:33<00:00,  2.97it/s, Train_Loss=0.02, Train_Accuracy=0.98840]\n",
      "Epoch 20/20 : Validating: 100%|█| 17/17 [00:05<00:00,  3.34it/s, Val_Loss=0.04410, Val_Accuarcy=0.97"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time 768.3078391551971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Start training, example with Dataset: MNIST-10; Precision=256; phase and intensity files are user-uploaded; \n",
    "#depth of the model is 2; system_size is 200; distance between layers is 0.6604m; \n",
    "#amp_factor is 50; learning rate is 0.7; training epoch is 20;\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((200),interpolation=2),transforms.ToTensor()])\n",
    "print(\"training and testing on MNIST10 dataset\")\n",
    "train_dataset = torchvision.datasets.MNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(\"./data\", train=False, transform=transform, download=True)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=600, num_workers=8, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=600, num_workers=8, shuffle=False, pin_memory=True)\n",
    "input_padding = 0\n",
    "\n",
    "phase_file = \"phase.csv\"\n",
    "phase_function = utils.phase_func(phase_file,  i_k=256)\n",
    "with open('phase_file.npy', 'wb') as f_phase:\n",
    "        np.save(f_phase, phase_function.cpu().numpy())\n",
    "intensity_file = \"intensity.csv\"\n",
    "intensity_function = utils.intensity_func(intensity_file,  i_k=256)\n",
    "with open('intensity_file.npy', 'wb') as f_amp:\n",
    "        np.save(f_amp, intensity_function.cpu().numpy())\n",
    "\n",
    "model = NetCodesign(num_layers=2, batch_norm =False, wavelength=5.32e-7, pixel_size=0.000036, sys_size=200, distance=0.6604,phase_func=phase_function, intensity_func=intensity_function, precision=256, amp_factor=50)\n",
    "model.cuda()\n",
    "lambda1= 1\n",
    "\n",
    "start_time = time()\n",
    "train(model, train_dataloader, val_dataloader, input_padding, lambda1)\n",
    "print('run time', time()-start_time)\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
